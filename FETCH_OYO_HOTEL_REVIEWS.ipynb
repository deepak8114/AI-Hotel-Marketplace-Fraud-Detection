{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7bb062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adc5453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def scrape_oyo_links(url):\n",
    "    \"\"\"\n",
    "    Scrapes OYO links from the provided URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to scrape the OYO links from.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the list of scraped URLs and link codes.\n",
    "    \"\"\"\n",
    "    # Code for scraping OYO links\n",
    "    logging.info('Executing scrape_oyo_links function')\n",
    "\n",
    "    # Configure Selenium options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "\n",
    "    # Set Chrome preferences to block images\n",
    "    prefs = {\n",
    "        'profile.managed_default_content_settings.images': 2\n",
    "    }\n",
    "    chrome_options.add_experimental_option('prefs', prefs)\n",
    "\n",
    "    # Set desired capabilities to block images\n",
    "    caps = DesiredCapabilities().CHROME\n",
    "    caps['pageLoadStrategy'] = 'none'\n",
    "\n",
    "    # Set path to your ChromeDriver executable\n",
    "    chrome_driver_path = \"/path/to/chromedriver\"\n",
    "\n",
    "    try:\n",
    "        # Create a new Selenium driver\n",
    "        driver = webdriver.Chrome(service=Service(chrome_driver_path), options=chrome_options, desired_capabilities=caps)\n",
    "\n",
    "        # Define the user agent string\n",
    "        user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "        # Set user agent for the driver\n",
    "        driver.execute_cdp_cmd('Network.setUserAgentOverride', {\"userAgent\": user_agent})\n",
    "\n",
    "        # Open the URL in the driver\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load\n",
    "        time.sleep(3)\n",
    "\n",
    "        if \"There is no property available for this search\" in driver.page_source:\n",
    "            logging.warning(\"No properties available for this search. Skipping...\")\n",
    "            return [], []\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                load_more_span = WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, \"//span[contains(text(), 'Results Found')]\")))\n",
    "                load_more_span.click()\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # Create a BeautifulSoup object to parse the HTML content\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        div_all_links = soup.find_all(\"a\", class_=\"c-nn640c u-width100\")\n",
    "\n",
    "        base_url = \"https://www.oyorooms.com/api/pwa/updateHotelCall?url=https%3A%2F%2Fbff.oyorooms.com%2Fv1%2Fhotels%2Freviews%3Fhotel_id%3D{hotel_id}%26ovh_property%3Dfalse%26limit%3D100&sort_option=&offset=\"\n",
    "        urls = []\n",
    "        link_codes = []\n",
    "\n",
    "        for link in div_all_links:\n",
    "            hotel_id = link.get(\"href\")\n",
    "            hotel_id_cleaned = hotel_id.replace(\"/\", \"\")\n",
    "            link_codes.append(hotel_id_cleaned)\n",
    "            url = base_url.format(hotel_id=hotel_id_cleaned)\n",
    "            urls.append(url)\n",
    "\n",
    "        return urls, link_codes\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in scrape_oyo_links function: {str(e)}\")\n",
    "        return [], []\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b4e79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def fetch_data(url):\n",
    "    \"\"\"\n",
    "    Fetches data from the provided URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to fetch the data from.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the fetched JSON data.\n",
    "    \"\"\"\n",
    "    logging.info('Executing fetch_data function')\n",
    "\n",
    "    # Define the user agent string\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "\n",
    "    # Send a GET request to the URL with the user agent in the header\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    data = []  # List to store the fetched JSON data\n",
    "    base_url = url\n",
    "\n",
    "    try:\n",
    "        # Make API calls to fetch data in batches\n",
    "        offset = 0\n",
    "        while True:\n",
    "            url = base_url + str(offset)\n",
    "\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                json_data = response.json()\n",
    "                data.append(json_data)  # Add the fetched JSON data to the list\n",
    "                next_offset = json_data['data'].get('next_offset')\n",
    "                \n",
    "                if next_offset is None:\n",
    "                    break  # If next_offset is None, break out of the loop\n",
    "\n",
    "                offset = next_offset\n",
    "            else:\n",
    "                logging.error(f\"Error occurred in API call with offset {offset}. Status code: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred in fetch_data function: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "816a4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def process_data(data, hotel_id, city_name):\n",
    "    \"\"\"\n",
    "    Processes the fetched data and extracts relevant information.\n",
    "\n",
    "    Args:\n",
    "        data (list): The fetched JSON data.\n",
    "        hotel_id (str): The ID of the hotel.\n",
    "        city_name (str): The name of the city.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the processed data.\n",
    "    \"\"\"\n",
    "    logging.info('Executing process_data function')\n",
    "\n",
    "    fetch_review = []\n",
    "    folder_path = os.path.join('json', city_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    json_file_path = os.path.join(folder_path, f'{hotel_id}.json')\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file)\n",
    "    \n",
    "    for json_obj in data:\n",
    "        reviews = json_obj['data']['reviews']\n",
    "        for review in reviews:\n",
    "            user_name = review.get('user_name') if review.get('user_name') else 'none'\n",
    "            review_text = review.get('review_text') if review.get('review_text') else 'none'\n",
    "            date = review.get('date') if review.get('date') else 'none'\n",
    "            title = review['rating'].get('title') if review['rating'].get('title') else 'none'\n",
    "            fetch_review.append([hotel_id, user_name, review_text, date, title])\n",
    "\n",
    "    return fetch_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd4a21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def save_hotel_reviews(hotel_id, processed_data, city_name):\n",
    "    \"\"\"\n",
    "    Saves the processed data as a CSV file.\n",
    "\n",
    "    Args:\n",
    "        hotel_id (str): The ID of the hotel.\n",
    "        processed_data (list): The processed data to be saved.\n",
    "        city_name (str): The name of the city.\n",
    "    \"\"\"\n",
    "    logging.info('Executing save_hotel_reviews function')\n",
    "\n",
    "    df = pd.DataFrame(processed_data, columns=['hotel_id', 'user_name', 'review_text', 'date', 'title'])\n",
    "\n",
    "    folder_name = city_name.replace(' ', '_')  # Replacing spaces with underscores in the city name\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    csv_filename = os.path.join(folder_name, f'{hotel_id}.csv')\n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ab157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "city_code_file = pd.read_csv('city_wise_url.csv')\n",
    "\n",
    "for index, city in city_code_file.iterrows():\n",
    "    logging.info(f\"Processing city {index + 1}/{len(city_code_file)}\")\n",
    "    city_name = city['name']\n",
    "    city_url = city['url']\n",
    "    result = scrape_oyo_links(city_url)\n",
    "    if not result:\n",
    "        logging.warning('No data available. Skipping...')\n",
    "        continue\n",
    "    urls, link_code = result\n",
    "\n",
    "    for url, hotel_id in zip(urls, link_code):\n",
    "        logging.info(f\"Processing hotel {hotel_id} in {city_name}\")\n",
    "        data = fetch_data(url)\n",
    "        processed_data = process_data(data, hotel_id, city_name)\n",
    "        save_hotel_reviews(hotel_id, processed_data, city_name)\n",
    "        logging.info(\"File saved successfully.\")\n",
    "\n",
    "logging.info(\"All cities and hotels processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a5ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a086cc16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
